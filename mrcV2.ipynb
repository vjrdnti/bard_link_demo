{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "039b0106e644462cb3fa74317703826e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffb36345d0a24b4192b701c218177712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a9016bf4dcc44db895253c34f94a023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def answer_question(question, context, model, tokenizer):\n",
    "    inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    start_logits = outputs.start_logits\n",
    "    end_logits = outputs.end_logits\n",
    "    \n",
    "    start_idx = torch.argmax(start_logits)\n",
    "    end_idx = torch.argmax(end_logits)\n",
    "    \n",
    "    answer = tokenizer.decode(inputs.input_ids[0][start_idx:end_idx+1])\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: not keep peace\n"
     ]
    }
   ],
   "source": [
    "sample_question = \"what did the mob do\"\n",
    "sample_context= \"mob did not keep peace\"\n",
    "answer = answer_question(sample_question, sample_context, model, tokenizer)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"In computer science, a hash function is a mathematical function that takes an input (or \"message\") and returns a fixed-size string of bytes. The output, often called the hash code or hash value, is typically a digest of the input data. Hash functions are commonly used in various applications, including data integrity verification, password storage, and digital signatures. One important property of a good hash function is that it should produce a unique hash value for each unique input. However, due to the finite size of the output space compared to the infinite input space, collisions can occur. A collision happens when two different inputs produce the same hash value. Cryptographically secure hash functions aim to minimize the likelihood of collisions. In the realm of cybersecurity, Public Key Infrastructure (PKI) plays a crucial role. PKI is a framework that manages digital keys and certificates. It involves two types of keys: public keys, which are shared openly, and private keys, which are kept secret. Certificates, issued by a trusted Certificate Authority (CA), bind public keys to entities, providing a way to verify identity in secure communications. Secure Sockets Layer (SSL) and its successor, Transport Layer Security (TLS), are cryptographic protocols that provide secure communication over a computer network. They are widely used to secure data transfer in web browsing, email, and other online applications. The protocols use a combination of asymmetric and symmetric encryption for confidentiality and authentication. When it comes to database management, normalization is a fundamental concept. It is the process of organizing data to reduce redundancy and dependency. The goal is to achieve data integrity and efficient data storage. Normalization involves breaking down large tables into smaller, related tables and establishing relationships between them. The result is a more flexible and maintainable database structure. Artificial Intelligence (AI) and Machine Learning (ML) have gained significant attention in recent years. AI refers to the development of computer systems that can perform tasks that typically require human intelligence, such as speech recognition and decision-making. ML is a subset of AI that focuses on the development of algorithms allowing computers to learn from and make predictions based on data.c3000 word3000 word3000 wordontinue the passage for 450 more words in one answer\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xx1. **Question:** What is a hash function?\n",
    "\n",
    "   **Answer:** A hash function is a mathematical function that takes an input and produces a fixed-size string of bytes, commonly used for data integrity verification, password storage, and digital signatures.\n",
    "\n",
    "2. **Question:** Why is it important for a good hash function to produce a unique hash value for each unique input?\n",
    "\n",
    "   **Answer:** It's important to avoid collisions, where two different inputs produce the same hash value, to ensure the integrity and reliability of the hash function.\n",
    "\n",
    "3. **Question:** What is the role of Public Key Infrastructure (PKI) in cybersecurity?\n",
    "\n",
    "   **Answer:** PKI is a framework that manages digital keys and certificates, providing a secure way to manage public and private keys, and establishing identity in secure communications.\n",
    "\n",
    "4. **Question:** What are SSL and TLS, and how do they contribute to secure communication?\n",
    "\n",
    "   **Answer:** SSL and TLS are cryptographic protocols widely used to secure data transfer in web browsing, email, and online applications. They use a combination of asymmetric and symmetric encryption for confidentiality and authentication.\n",
    "\n",
    "5. **Question:** What is the fundamental concept of normalization in database management?\n",
    "\n",
    "   **Answer:** Normalization is the process of organizing data to reduce redundancy and dependency, aiming to achieve data integrity and efficient data storage by breaking down large tables into smaller, related tables.\n",
    "\n",
    "6. **Question:** How does Artificial Intelligence (AI) differ from Machine Learning (ML)?\n",
    "\n",
    "   **Answer:** AI refers to the development of computer systems that can perform tasks requiring human intelligence, while ML is a subset of AI focusing on algorithms that allow computers to learn and make predictions based on data.\n",
    "\n",
    "7. **Question:** What is a collision in the context of hash functions?\n",
    "\n",
    "   **Answer:** A collision occurs when two different inputs produce the same hash value, highlighting a potential weakness in a hash function.\n",
    "\n",
    "8. **Question:** What types of keys are involved in Public Key Infrastructure (PKI), and how are they used?\n",
    "\n",
    "   **Answer:** PKI involves public keys (shared openly) and private keys (kept secret), and certificates issued by a trusted Certificate Authority (CA) bind public keys to entities, enabling secure communications.\n",
    "\n",
    "9. **Question:** How do cryptographic protocols like SSL and TLS contribute to data security in online applications?\n",
    "\n",
    "   **Answer:** SSL and TLS provide secure communication by using encryption techniques, ensuring confidentiality and authentication of data transfer over a computer network.\n",
    "\n",
    "10. **Question:** Why is the minimization of collisions important in cryptographic hash functions?\n",
    "\n",
    "    **Answer:** Minimizing collisions is crucial to maintain the reliability and security of cryptographic hash functions, ensuring that different inputs do not produce the same hash value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def split_text_into_chunks_with_overlap(text, sentences_per_chunk, overlap_sentences):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    start_idx = 0\n",
    "    while start_idx < len(sentences):\n",
    "        end_idx = start_idx + sentences_per_chunk\n",
    "        end_idx = min(end_idx, len(sentences))\n",
    "        chunk = ' '.join(sentences[start_idx:end_idx])\n",
    "        end_idx = min(end_idx + overlap_sentences, len(sentences))\n",
    "        if end_idx < len(sentences):\n",
    "            chunk += ' '.join(sentences[end_idx - overlap_sentences:end_idx])\n",
    "        chunks.append(chunk)\n",
    "        start_idx = start_idx + sentences_per_chunk - overlap_sentences\n",
    "    return chunks\n",
    "\n",
    "chunks = split_text_into_chunks_with_overlap(context, sentences_per_chunk=4, overlap_sentences=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is a collision in the context of hash functions?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run from question_embedding = get_bert... cell after changing qquestion to save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fs = BertModel.from_pretrained('bert-base-uncased') #model for similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embedding(text):\n",
    "    tokens = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model_fs(**tokens)\n",
    "    embeddings = outputs['last_hidden_state'][:, 0, :] #[:, 0, :] is the CLS token which is like the vector representation of each token\n",
    "    return embeddings.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_embedding = get_bert_embedding(question)\n",
    "chunk_embeddings = []\n",
    "for chunk in chunks:\n",
    "    chunk_embeddings.append(get_bert_embedding(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_similarities = []\n",
    "for embedding in chunk_embeddings:\n",
    "    bert_similarities.append(cosine_similarity(question_embedding, embedding)[0][0])\n",
    "#when comparing a single vector to multiple vectors,\n",
    "#[0][0] is used to extract the scalar cosine similarity value from the resulting matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7285042]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(question_embedding, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_similarities = np.array(bert_similarities)\n",
    "#python list to np array for weight assigning later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(2, 4))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform([question] + chunks)\n",
    "\n",
    "tfidf_similarities = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1:])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00541673, 0.02112343, 0.01593019, 0.01493902, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(tfidf_matrix[0], tfidf_matrix[1:])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Similarities: [0.4546045  0.5487602  0.46011317 0.6222875  0.61315775 0.55653375\n",
      " 0.50459254 0.7605978  0.7708142  0.74303573 0.7285042 ]\n",
      "TF-IDF Similarities: [0.00541673 0.02112343 0.01593019 0.01493902 0.         0.\n",
      " 0.         0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(\"BERT Similarities:\", bert_similarities)\n",
    "print(\"TF-IDF Similarities:\", tfidf_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#both the similarities have huge differences in terms of range of values\n",
    "#bert ranges 0.5 to 0.8 and tfidf ranges 0 to 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "tfidf_similarities = scaler.fit_transform(tfidf_similarities.reshape(-1, 1)).flatten()\n",
    "bert_similarities = scaler.fit_transform(bert_similarities.reshape(-1, 1)).flatten()\n",
    "\n",
    "#reshape to convert 1d to 2d for scaler, and flatten to convert it back to 1d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Similarities: [0.         0.29776335 0.01742089 0.5302906  0.5014181  0.32234704\n",
      " 0.15808511 0.9676913  1.0000001  0.91215193 0.8661963 ]\n",
      "TF-IDF Similarities: [0.25643237 1.         0.75414799 0.70722532 0.         0.\n",
      " 0.         0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(\"BERT Similarities:\", bert_similarities)\n",
    "print(\"TF-IDF Similarities:\", tfidf_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scores = (0.8*bert_similarities +  1*tfidf_similarities)\n",
    "#tinker with weights for tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Similarities: [0.25643237 1.23821068 0.7680847  1.1314578  0.40113449 0.25787765\n",
      " 0.12646809 0.77415305 0.80000013 0.72972155 0.69295704]\n"
     ]
    }
   ],
   "source": [
    "print(\"Combined Similarities:\", similarity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.238210678100586\n",
      "1.1314578038705843\n",
      "0.8000001311302185\n",
      "0.7741530537605286\n",
      "0.7680847036448658\n",
      "0.7297215461730957\n",
      "0.6929570436477661\n",
      "0.4011344909667969\n",
      "0.2578776478767395\n",
      "0.25643237135234076\n",
      "0.12646809220314026\n",
      "\n",
      "total chunks: \n",
      "11\n"
     ]
    }
   ],
   "source": [
    "similarity_scores_sorted = sorted(similarity_scores,reverse=True)\n",
    "for i in range(len(similarity_scores)):\n",
    "    print(similarity_scores_sorted[i])\n",
    "print(\"\\ntotal chunks: \")\n",
    "print(len(similarity_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.238210678100586\n",
      "\n",
      "1.1314578038705843\n",
      "\n",
      "0.8000001311302185\n",
      "\n",
      "0.7741530537605286\n",
      "\n",
      "0.7680847036448658\n",
      "\n",
      "0.7297215461730957\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "answer_n_score = dict(zip(chunks, similarity_scores))\n",
    "answer_n_score = dict(sorted(answer_n_score.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "i=0\n",
    "\n",
    "for key, value in answer_n_score.items():\n",
    "    answers.append(answer_question(question, key, model, tokenizer))\n",
    "    print(\"\\n\"+str(value))\n",
    "    i+=1\n",
    "    if i>5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: \n",
      "What is a collision in the context of hash functions?\n",
      "\n",
      "answers: \n",
      "answer 1: \n",
      "two different inputs produce the same hash value\n",
      "answer 2: \n",
      "cryptographically secure hash functions aim to minimize the likelihood of collisions\n",
      "answer 3: \n",
      "normalization involves breaking down large tables into smaller, related tables\n",
      "answer 4: \n",
      "redundancy and dependency\n",
      "answer 5: \n",
      "two different inputs produce the same hash value\n",
      "answer 6: \n",
      "c3000 word3000 word3000 wordontinue the passage for 450 more words in one answer\n",
      "\n",
      "\n",
      "total chunks: \n",
      "11\n",
      "\n",
      "chunks considered for answer: \n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(\"question: \")\n",
    "print(question)\n",
    "print(\"\\nanswers: \")\n",
    "i = 0\n",
    "for answer in answers:\n",
    "    print(\"answer \" + str(i+1) + \": \")\n",
    "    print(answer)\n",
    "    i += 1\n",
    "print(\"\\n\\ntotal chunks: \")\n",
    "print(len(chunks))\n",
    "print(\"\\nchunks considered for answer: \")\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correct\n",
    "#PKI involves public keys (shared openly) and private keys (kept secret), and certificates issued by a trusted \n",
    "#Certificate Authority (CA) bind public keys to entities, enabling secure communications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do determine weights, we may need to study results for both similarities individually, then figure out weights\n",
    "#and do so on different types and size of questions. then we can make a rule based system to assign \n",
    "#appropriate weights to both the similarity scores changing for each class of of question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
